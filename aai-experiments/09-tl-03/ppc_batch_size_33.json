{"test__classification_error": 0.2803030386567116, "is_num_iterations_reached": 0, "done": false, "training_iteration": 1, "trial_id": "b87ab_00000", "date": "2024-06-30_18-58-50", "timestamp": 1719773930, "time_this_iter_s": 16.299274444580078, "time_total_s": 16.299274444580078, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 16.299274444580078, "iterations_since_restore": 1}
{"test__classification_error": 0.22727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 2, "trial_id": "b87ab_00000", "date": "2024-06-30_18-59-05", "timestamp": 1719773945, "time_this_iter_s": 14.929507732391357, "time_total_s": 31.228782176971436, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 31.228782176971436, "iterations_since_restore": 2}
{"test__classification_error": 0.34090910106897354, "is_num_iterations_reached": 0, "done": false, "training_iteration": 3, "trial_id": "b87ab_00000", "date": "2024-06-30_18-59-23", "timestamp": 1719773963, "time_this_iter_s": 17.74595808982849, "time_total_s": 48.97474026679993, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 48.97474026679993, "iterations_since_restore": 3}
{"test__classification_error": 0.29545455425977707, "is_num_iterations_reached": 0, "done": false, "training_iteration": 4, "trial_id": "b87ab_00000", "date": "2024-06-30_18-59-39", "timestamp": 1719773979, "time_this_iter_s": 16.252089738845825, "time_total_s": 65.22683000564575, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 65.22683000564575, "iterations_since_restore": 4}
{"test__classification_error": 0.20454546064138412, "is_num_iterations_reached": 0, "done": false, "training_iteration": 5, "trial_id": "b87ab_00000", "date": "2024-06-30_18-59-57", "timestamp": 1719773997, "time_this_iter_s": 17.508369207382202, "time_total_s": 82.73519921302795, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 82.73519921302795, "iterations_since_restore": 5}
{"test__classification_error": 0.17424242943525314, "is_num_iterations_reached": 0, "done": false, "training_iteration": 6, "trial_id": "b87ab_00000", "date": "2024-06-30_19-00-16", "timestamp": 1719774016, "time_this_iter_s": 19.053252935409546, "time_total_s": 101.7884521484375, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 101.7884521484375, "iterations_since_restore": 6}
{"test__classification_error": 0.2348484918475151, "is_num_iterations_reached": 0, "done": false, "training_iteration": 7, "trial_id": "b87ab_00000", "date": "2024-06-30_19-00-35", "timestamp": 1719774035, "time_this_iter_s": 19.52421522140503, "time_total_s": 121.31266736984253, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 121.31266736984253, "iterations_since_restore": 7}
{"test__classification_error": 0.2348484918475151, "is_num_iterations_reached": 0, "done": false, "training_iteration": 8, "trial_id": "b87ab_00000", "date": "2024-06-30_19-00-56", "timestamp": 1719774056, "time_this_iter_s": 20.64181423187256, "time_total_s": 141.9544816017151, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 141.9544816017151, "iterations_since_restore": 8}
{"test__classification_error": 0.18939394503831863, "is_num_iterations_reached": 0, "done": false, "training_iteration": 9, "trial_id": "b87ab_00000", "date": "2024-06-30_19-01-14", "timestamp": 1719774074, "time_this_iter_s": 18.586110591888428, "time_total_s": 160.54059219360352, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 160.54059219360352, "iterations_since_restore": 9}
{"test__classification_error": 0.29545455425977707, "is_num_iterations_reached": 0, "done": false, "training_iteration": 10, "trial_id": "b87ab_00000", "date": "2024-06-30_19-01-36", "timestamp": 1719774096, "time_this_iter_s": 21.208839654922485, "time_total_s": 181.749431848526, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 181.749431848526, "iterations_since_restore": 10}
{"test__classification_error": 0.2803030386567116, "is_num_iterations_reached": 0, "done": false, "training_iteration": 11, "trial_id": "b87ab_00000", "date": "2024-06-30_19-01-51", "timestamp": 1719774111, "time_this_iter_s": 15.816135883331299, "time_total_s": 197.5655677318573, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 197.5655677318573, "iterations_since_restore": 11}
{"test__classification_error": 0.21212121844291687, "is_num_iterations_reached": 0, "done": false, "training_iteration": 12, "trial_id": "b87ab_00000", "date": "2024-06-30_19-02-11", "timestamp": 1719774131, "time_this_iter_s": 19.842255115509033, "time_total_s": 217.40782284736633, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 217.40782284736633, "iterations_since_restore": 12}
{"test__classification_error": 0.1818181872367859, "is_num_iterations_reached": 0, "done": false, "training_iteration": 13, "trial_id": "b87ab_00000", "date": "2024-06-30_19-02-27", "timestamp": 1719774147, "time_this_iter_s": 15.247850894927979, "time_total_s": 232.6556737422943, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 232.6556737422943, "iterations_since_restore": 13}
{"test__classification_error": 0.19696970283985138, "is_num_iterations_reached": 0, "done": false, "training_iteration": 14, "trial_id": "b87ab_00000", "date": "2024-06-30_19-02-44", "timestamp": 1719774164, "time_this_iter_s": 17.602938652038574, "time_total_s": 250.25861239433289, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 250.25861239433289, "iterations_since_restore": 14}
{"test__classification_error": 0.1818181872367859, "is_num_iterations_reached": 0, "done": false, "training_iteration": 15, "trial_id": "b87ab_00000", "date": "2024-06-30_19-03-01", "timestamp": 1719774181, "time_this_iter_s": 17.078239679336548, "time_total_s": 267.33685207366943, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 267.33685207366943, "iterations_since_restore": 15}
{"test__classification_error": 0.1818181872367859, "is_num_iterations_reached": 0, "done": false, "training_iteration": 16, "trial_id": "b87ab_00000", "date": "2024-06-30_19-03-17", "timestamp": 1719774197, "time_this_iter_s": 15.50996470451355, "time_total_s": 282.846816778183, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 282.846816778183, "iterations_since_restore": 16}
{"test__classification_error": 0.2348484918475151, "is_num_iterations_reached": 0, "done": false, "training_iteration": 17, "trial_id": "b87ab_00000", "date": "2024-06-30_19-03-36", "timestamp": 1719774216, "time_this_iter_s": 18.69721508026123, "time_total_s": 301.5440318584442, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 301.5440318584442, "iterations_since_restore": 17}
{"test__classification_error": 0.21212121844291687, "is_num_iterations_reached": 0, "done": false, "training_iteration": 18, "trial_id": "b87ab_00000", "date": "2024-06-30_19-03-50", "timestamp": 1719774230, "time_this_iter_s": 14.611401796340942, "time_total_s": 316.15543365478516, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 316.15543365478516, "iterations_since_restore": 18}
{"test__classification_error": 0.17424242943525314, "is_num_iterations_reached": 0, "done": false, "training_iteration": 19, "trial_id": "b87ab_00000", "date": "2024-06-30_19-04-08", "timestamp": 1719774248, "time_this_iter_s": 18.12264657020569, "time_total_s": 334.27808022499084, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 334.27808022499084, "iterations_since_restore": 19}
{"test__classification_error": 0.19696970283985138, "is_num_iterations_reached": 0, "done": false, "training_iteration": 20, "trial_id": "b87ab_00000", "date": "2024-06-30_19-04-26", "timestamp": 1719774266, "time_this_iter_s": 18.1679470539093, "time_total_s": 352.44602727890015, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 352.44602727890015, "iterations_since_restore": 20}
{"test__classification_error": 0.17424242943525314, "is_num_iterations_reached": 0, "done": false, "training_iteration": 21, "trial_id": "b87ab_00000", "date": "2024-06-30_19-04-45", "timestamp": 1719774285, "time_this_iter_s": 18.39316463470459, "time_total_s": 370.83919191360474, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 370.83919191360474, "iterations_since_restore": 21}
{"test__classification_error": 0.19696970283985138, "is_num_iterations_reached": 0, "done": false, "training_iteration": 22, "trial_id": "b87ab_00000", "date": "2024-06-30_19-05-02", "timestamp": 1719774302, "time_this_iter_s": 16.94819974899292, "time_total_s": 387.78739166259766, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 387.78739166259766, "iterations_since_restore": 22}
{"test__classification_error": 0.1666666716337204, "is_num_iterations_reached": 0, "done": false, "training_iteration": 23, "trial_id": "b87ab_00000", "date": "2024-06-30_19-05-19", "timestamp": 1719774319, "time_this_iter_s": 17.355578660964966, "time_total_s": 405.1429703235626, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 405.1429703235626, "iterations_since_restore": 23}
{"test__classification_error": 0.19696970283985138, "is_num_iterations_reached": 0, "done": false, "training_iteration": 24, "trial_id": "b87ab_00000", "date": "2024-06-30_19-05-35", "timestamp": 1719774335, "time_this_iter_s": 16.083987951278687, "time_total_s": 421.2269582748413, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 421.2269582748413, "iterations_since_restore": 24}
{"test__classification_error": 0.1666666716337204, "is_num_iterations_reached": 0, "done": false, "training_iteration": 25, "trial_id": "b87ab_00000", "date": "2024-06-30_19-05-51", "timestamp": 1719774351, "time_this_iter_s": 15.413938283920288, "time_total_s": 436.6408965587616, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 436.6408965587616, "iterations_since_restore": 25}
{"test__classification_error": 0.1666666716337204, "is_num_iterations_reached": 0, "done": false, "training_iteration": 26, "trial_id": "b87ab_00000", "date": "2024-06-30_19-06-09", "timestamp": 1719774369, "time_this_iter_s": 18.303239107131958, "time_total_s": 454.94413566589355, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 454.94413566589355, "iterations_since_restore": 26}
{"test__classification_error": 0.19696970283985138, "is_num_iterations_reached": 0, "done": false, "training_iteration": 27, "trial_id": "b87ab_00000", "date": "2024-06-30_19-06-24", "timestamp": 1719774384, "time_this_iter_s": 15.355591773986816, "time_total_s": 470.29972743988037, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 470.29972743988037, "iterations_since_restore": 27}
{"test__classification_error": 0.2348484918475151, "is_num_iterations_reached": 0, "done": false, "training_iteration": 28, "trial_id": "b87ab_00000", "date": "2024-06-30_19-06-31", "timestamp": 1719774391, "time_this_iter_s": 7.014620780944824, "time_total_s": 477.3143482208252, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 477.3143482208252, "iterations_since_restore": 28}
{"test__classification_error": 0.2878787964582443, "is_num_iterations_reached": 0, "done": false, "training_iteration": 29, "trial_id": "b87ab_00000", "date": "2024-06-30_19-06-43", "timestamp": 1719774403, "time_this_iter_s": 11.126845359802246, "time_total_s": 488.44119358062744, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 488.44119358062744, "iterations_since_restore": 29}
{"test__classification_error": 0.2878787964582443, "is_num_iterations_reached": 0, "done": false, "training_iteration": 30, "trial_id": "b87ab_00000", "date": "2024-06-30_19-06-52", "timestamp": 1719774412, "time_this_iter_s": 9.47337293624878, "time_total_s": 497.9145665168762, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 497.9145665168762, "iterations_since_restore": 30}
{"test__classification_error": 0.1818181872367859, "is_num_iterations_reached": 0, "done": false, "training_iteration": 31, "trial_id": "b87ab_00000", "date": "2024-06-30_19-07-01", "timestamp": 1719774421, "time_this_iter_s": 8.911665916442871, "time_total_s": 506.8262324333191, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 506.8262324333191, "iterations_since_restore": 31}
{"test__classification_error": 0.19696970283985138, "is_num_iterations_reached": 0, "done": false, "training_iteration": 32, "trial_id": "b87ab_00000", "date": "2024-06-30_19-07-12", "timestamp": 1719774432, "time_this_iter_s": 10.753417491912842, "time_total_s": 517.5796499252319, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 517.5796499252319, "iterations_since_restore": 32}
{"test__classification_error": 0.1515151560306549, "is_num_iterations_reached": 0, "done": false, "training_iteration": 33, "trial_id": "b87ab_00000", "date": "2024-06-30_19-07-24", "timestamp": 1719774444, "time_this_iter_s": 12.226443767547607, "time_total_s": 529.8060936927795, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 529.8060936927795, "iterations_since_restore": 33}
{"test__classification_error": 0.21212121844291687, "is_num_iterations_reached": 0, "done": false, "training_iteration": 34, "trial_id": "b87ab_00000", "date": "2024-06-30_19-07-32", "timestamp": 1719774452, "time_this_iter_s": 8.089387655258179, "time_total_s": 537.8954813480377, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 537.8954813480377, "iterations_since_restore": 34}
{"test__classification_error": 0.1666666716337204, "is_num_iterations_reached": 0, "done": false, "training_iteration": 35, "trial_id": "b87ab_00000", "date": "2024-06-30_19-07-43", "timestamp": 1719774463, "time_this_iter_s": 10.888619899749756, "time_total_s": 548.7841012477875, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 548.7841012477875, "iterations_since_restore": 35}
{"test__classification_error": 0.17424242943525314, "is_num_iterations_reached": 0, "done": false, "training_iteration": 36, "trial_id": "b87ab_00000", "date": "2024-06-30_19-07-53", "timestamp": 1719774473, "time_this_iter_s": 9.703285455703735, "time_total_s": 558.4873867034912, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 558.4873867034912, "iterations_since_restore": 36}
{"test__classification_error": 0.22727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 37, "trial_id": "b87ab_00000", "date": "2024-06-30_19-08-08", "timestamp": 1719774488, "time_this_iter_s": 15.136865854263306, "time_total_s": 573.6242525577545, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 573.6242525577545, "iterations_since_restore": 37}
{"test__classification_error": 0.13636364042758942, "is_num_iterations_reached": 0, "done": false, "training_iteration": 38, "trial_id": "b87ab_00000", "date": "2024-06-30_19-08-20", "timestamp": 1719774500, "time_this_iter_s": 11.894049644470215, "time_total_s": 585.5183022022247, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 585.5183022022247, "iterations_since_restore": 38}
{"test__classification_error": 0.2803030386567116, "is_num_iterations_reached": 0, "done": false, "training_iteration": 39, "trial_id": "b87ab_00000", "date": "2024-06-30_19-08-27", "timestamp": 1719774507, "time_this_iter_s": 7.52569842338562, "time_total_s": 593.0440006256104, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 593.0440006256104, "iterations_since_restore": 39}
{"test__classification_error": 0.1666666716337204, "is_num_iterations_reached": 0, "done": false, "training_iteration": 40, "trial_id": "b87ab_00000", "date": "2024-06-30_19-08-36", "timestamp": 1719774516, "time_this_iter_s": 8.904480218887329, "time_total_s": 601.9484808444977, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 601.9484808444977, "iterations_since_restore": 40}
{"test__classification_error": 0.3712121322751045, "is_num_iterations_reached": 0, "done": false, "training_iteration": 41, "trial_id": "b87ab_00000", "date": "2024-06-30_19-08-46", "timestamp": 1719774526, "time_this_iter_s": 9.698014736175537, "time_total_s": 611.6464955806732, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 611.6464955806732, "iterations_since_restore": 41}
{"test__classification_error": 0.2878787964582443, "is_num_iterations_reached": 1, "done": true, "training_iteration": 42, "trial_id": "b87ab_00000", "date": "2024-06-30_19-09-01", "timestamp": 1719774541, "time_this_iter_s": 14.889747858047485, "time_total_s": 626.5362434387207, "pid": 9261, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 626.5362434387207, "iterations_since_restore": 42}
