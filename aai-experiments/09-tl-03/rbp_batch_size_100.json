{"test__classification_error": 0.17000000178813934, "is_num_iterations_reached": 0, "done": false, "training_iteration": 1, "trial_id": "89ad9_00001", "date": "2024-06-30_18-36-01", "timestamp": 1719772561, "time_this_iter_s": 10.311256647109985, "time_total_s": 10.311256647109985, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 10.311256647109985, "iterations_since_restore": 1}
{"test__classification_error": 0.09999999403953552, "is_num_iterations_reached": 0, "done": false, "training_iteration": 2, "trial_id": "89ad9_00001", "date": "2024-06-30_18-36-07", "timestamp": 1719772567, "time_this_iter_s": 6.191810131072998, "time_total_s": 16.503066778182983, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 16.503066778182983, "iterations_since_restore": 2}
{"test__classification_error": 0.07999999821186066, "is_num_iterations_reached": 0, "done": false, "training_iteration": 3, "trial_id": "89ad9_00001", "date": "2024-06-30_18-36-15", "timestamp": 1719772575, "time_this_iter_s": 8.202456951141357, "time_total_s": 24.70552372932434, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 24.70552372932434, "iterations_since_restore": 3}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 4, "trial_id": "89ad9_00001", "date": "2024-06-30_18-36-21", "timestamp": 1719772581, "time_this_iter_s": 6.285465717315674, "time_total_s": 30.990989446640015, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 30.990989446640015, "iterations_since_restore": 4}
{"test__classification_error": 0.04999999701976776, "is_num_iterations_reached": 0, "done": false, "training_iteration": 5, "trial_id": "89ad9_00001", "date": "2024-06-30_18-36-29", "timestamp": 1719772589, "time_this_iter_s": 8.270099639892578, "time_total_s": 39.26108908653259, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 39.26108908653259, "iterations_since_restore": 5}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 6, "trial_id": "89ad9_00001", "date": "2024-06-30_18-36-37", "timestamp": 1719772597, "time_this_iter_s": 7.461310863494873, "time_total_s": 46.722399950027466, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 46.722399950027466, "iterations_since_restore": 6}
{"test__classification_error": 0.03999999910593033, "is_num_iterations_reached": 0, "done": false, "training_iteration": 7, "trial_id": "89ad9_00001", "date": "2024-06-30_18-36-44", "timestamp": 1719772604, "time_this_iter_s": 7.4366796016693115, "time_total_s": 54.15907955169678, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 54.15907955169678, "iterations_since_restore": 7}
{"test__classification_error": 0.019999999552965164, "is_num_iterations_reached": 0, "done": false, "training_iteration": 8, "trial_id": "89ad9_00001", "date": "2024-06-30_18-36-52", "timestamp": 1719772612, "time_this_iter_s": 7.179685831069946, "time_total_s": 61.338765382766724, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 61.338765382766724, "iterations_since_restore": 8}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 9, "trial_id": "89ad9_00001", "date": "2024-06-30_18-36-58", "timestamp": 1719772618, "time_this_iter_s": 6.20477032661438, "time_total_s": 67.5435357093811, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 67.5435357093811, "iterations_since_restore": 9}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 10, "trial_id": "89ad9_00001", "date": "2024-06-30_18-37-06", "timestamp": 1719772626, "time_this_iter_s": 8.491573333740234, "time_total_s": 76.03510904312134, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 76.03510904312134, "iterations_since_restore": 10}
{"test__classification_error": 0.019999999552965164, "is_num_iterations_reached": 0, "done": false, "training_iteration": 11, "trial_id": "89ad9_00001", "date": "2024-06-30_18-37-13", "timestamp": 1719772633, "time_this_iter_s": 6.2103590965271, "time_total_s": 82.24546813964844, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 82.24546813964844, "iterations_since_restore": 11}
{"test__classification_error": 0.009999999776482582, "is_num_iterations_reached": 0, "done": false, "training_iteration": 12, "trial_id": "89ad9_00001", "date": "2024-06-30_18-37-22", "timestamp": 1719772642, "time_this_iter_s": 9.07089376449585, "time_total_s": 91.31636190414429, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 91.31636190414429, "iterations_since_restore": 12}
{"test__classification_error": 0.009999999776482582, "is_num_iterations_reached": 0, "done": false, "training_iteration": 13, "trial_id": "89ad9_00001", "date": "2024-06-30_18-37-27", "timestamp": 1719772647, "time_this_iter_s": 5.828948020935059, "time_total_s": 97.14530992507935, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 97.14530992507935, "iterations_since_restore": 13}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 14, "trial_id": "89ad9_00001", "date": "2024-06-30_18-37-37", "timestamp": 1719772657, "time_this_iter_s": 9.13951063156128, "time_total_s": 106.28482055664062, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 106.28482055664062, "iterations_since_restore": 14}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 15, "trial_id": "89ad9_00001", "date": "2024-06-30_18-37-42", "timestamp": 1719772662, "time_this_iter_s": 5.623512506484985, "time_total_s": 111.90833306312561, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 111.90833306312561, "iterations_since_restore": 15}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 16, "trial_id": "89ad9_00001", "date": "2024-06-30_18-37-50", "timestamp": 1719772670, "time_this_iter_s": 7.567501783370972, "time_total_s": 119.47583484649658, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 119.47583484649658, "iterations_since_restore": 16}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 17, "trial_id": "89ad9_00001", "date": "2024-06-30_18-37-58", "timestamp": 1719772678, "time_this_iter_s": 7.723872661590576, "time_total_s": 127.19970750808716, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 127.19970750808716, "iterations_since_restore": 17}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 18, "trial_id": "89ad9_00001", "date": "2024-06-30_18-38-05", "timestamp": 1719772685, "time_this_iter_s": 7.674730539321899, "time_total_s": 134.87443804740906, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 134.87443804740906, "iterations_since_restore": 18}
{"test__classification_error": 0.009999999776482582, "is_num_iterations_reached": 0, "done": false, "training_iteration": 19, "trial_id": "89ad9_00001", "date": "2024-06-30_18-38-14", "timestamp": 1719772694, "time_this_iter_s": 8.273905277252197, "time_total_s": 143.14834332466125, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 143.14834332466125, "iterations_since_restore": 19}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 20, "trial_id": "89ad9_00001", "date": "2024-06-30_18-38-23", "timestamp": 1719772703, "time_this_iter_s": 9.38666296005249, "time_total_s": 152.53500628471375, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 152.53500628471375, "iterations_since_restore": 20}
{"test__classification_error": 0.009999999776482582, "is_num_iterations_reached": 0, "done": false, "training_iteration": 21, "trial_id": "89ad9_00001", "date": "2024-06-30_18-38-29", "timestamp": 1719772709, "time_this_iter_s": 6.1230692863464355, "time_total_s": 158.65807557106018, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 158.65807557106018, "iterations_since_restore": 21}
{"test__classification_error": 0.03999999910593033, "is_num_iterations_reached": 0, "done": false, "training_iteration": 22, "trial_id": "89ad9_00001", "date": "2024-06-30_18-38-35", "timestamp": 1719772715, "time_this_iter_s": 5.962439775466919, "time_total_s": 164.6205153465271, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 164.6205153465271, "iterations_since_restore": 22}
{"test__classification_error": 0.019999999552965164, "is_num_iterations_reached": 0, "done": false, "training_iteration": 23, "trial_id": "89ad9_00001", "date": "2024-06-30_18-38-43", "timestamp": 1719772723, "time_this_iter_s": 8.369439601898193, "time_total_s": 172.9899549484253, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 172.9899549484253, "iterations_since_restore": 23}
{"test__classification_error": 0.019999999552965164, "is_num_iterations_reached": 0, "done": false, "training_iteration": 24, "trial_id": "89ad9_00001", "date": "2024-06-30_18-38-49", "timestamp": 1719772729, "time_this_iter_s": 5.806577682495117, "time_total_s": 178.7965326309204, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 178.7965326309204, "iterations_since_restore": 24}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 25, "trial_id": "89ad9_00001", "date": "2024-06-30_18-38-58", "timestamp": 1719772738, "time_this_iter_s": 8.577985048294067, "time_total_s": 187.37451767921448, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 187.37451767921448, "iterations_since_restore": 25}
{"test__classification_error": 0.03999999910593033, "is_num_iterations_reached": 0, "done": false, "training_iteration": 26, "trial_id": "89ad9_00001", "date": "2024-06-30_18-39-04", "timestamp": 1719772744, "time_this_iter_s": 5.808074235916138, "time_total_s": 193.18259191513062, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 193.18259191513062, "iterations_since_restore": 26}
{"test__classification_error": 0.009999999776482582, "is_num_iterations_reached": 0, "done": false, "training_iteration": 27, "trial_id": "89ad9_00001", "date": "2024-06-30_18-39-12", "timestamp": 1719772752, "time_this_iter_s": 8.00180435180664, "time_total_s": 201.18439626693726, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 201.18439626693726, "iterations_since_restore": 27}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 28, "trial_id": "89ad9_00001", "date": "2024-06-30_18-39-19", "timestamp": 1719772759, "time_this_iter_s": 6.9143054485321045, "time_total_s": 208.09870171546936, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 208.09870171546936, "iterations_since_restore": 28}
{"test__classification_error": 0.03999999910593033, "is_num_iterations_reached": 0, "done": false, "training_iteration": 29, "trial_id": "89ad9_00001", "date": "2024-06-30_18-39-25", "timestamp": 1719772765, "time_this_iter_s": 6.458584547042847, "time_total_s": 214.5572862625122, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 214.5572862625122, "iterations_since_restore": 29}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 30, "trial_id": "89ad9_00001", "date": "2024-06-30_18-39-33", "timestamp": 1719772773, "time_this_iter_s": 8.253096103668213, "time_total_s": 222.81038236618042, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 222.81038236618042, "iterations_since_restore": 30}
{"test__classification_error": 0.03999999910593033, "is_num_iterations_reached": 0, "done": false, "training_iteration": 31, "trial_id": "89ad9_00001", "date": "2024-06-30_18-39-40", "timestamp": 1719772780, "time_this_iter_s": 6.977446794509888, "time_total_s": 229.7878291606903, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 229.7878291606903, "iterations_since_restore": 31}
{"test__classification_error": 0.03999999910593033, "is_num_iterations_reached": 0, "done": false, "training_iteration": 32, "trial_id": "89ad9_00001", "date": "2024-06-30_18-39-49", "timestamp": 1719772789, "time_this_iter_s": 9.03257417678833, "time_total_s": 238.82040333747864, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 238.82040333747864, "iterations_since_restore": 32}
{"test__classification_error": 0.019999999552965164, "is_num_iterations_reached": 0, "done": false, "training_iteration": 33, "trial_id": "89ad9_00001", "date": "2024-06-30_18-39-55", "timestamp": 1719772795, "time_this_iter_s": 5.968936204910278, "time_total_s": 244.78933954238892, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 244.78933954238892, "iterations_since_restore": 33}
{"test__classification_error": 0.019999999552965164, "is_num_iterations_reached": 0, "done": false, "training_iteration": 34, "trial_id": "89ad9_00001", "date": "2024-06-30_18-40-04", "timestamp": 1719772804, "time_this_iter_s": 8.49771499633789, "time_total_s": 253.2870545387268, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 253.2870545387268, "iterations_since_restore": 34}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 35, "trial_id": "89ad9_00001", "date": "2024-06-30_18-40-11", "timestamp": 1719772811, "time_this_iter_s": 6.778128385543823, "time_total_s": 260.06518292427063, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 260.06518292427063, "iterations_since_restore": 35}
{"test__classification_error": 0.019999999552965164, "is_num_iterations_reached": 0, "done": false, "training_iteration": 36, "trial_id": "89ad9_00001", "date": "2024-06-30_18-40-19", "timestamp": 1719772819, "time_this_iter_s": 8.715734958648682, "time_total_s": 268.7809178829193, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 268.7809178829193, "iterations_since_restore": 36}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 0, "done": false, "training_iteration": 37, "trial_id": "89ad9_00001", "date": "2024-06-30_18-40-26", "timestamp": 1719772826, "time_this_iter_s": 6.388834476470947, "time_total_s": 275.16975235939026, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 275.16975235939026, "iterations_since_restore": 37}
{"test__classification_error": 0.019999999552965164, "is_num_iterations_reached": 0, "done": false, "training_iteration": 38, "trial_id": "89ad9_00001", "date": "2024-06-30_18-40-34", "timestamp": 1719772834, "time_this_iter_s": 8.29175615310669, "time_total_s": 283.46150851249695, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 283.46150851249695, "iterations_since_restore": 38}
{"test__classification_error": 0.009999999776482582, "is_num_iterations_reached": 0, "done": false, "training_iteration": 39, "trial_id": "89ad9_00001", "date": "2024-06-30_18-40-44", "timestamp": 1719772844, "time_this_iter_s": 9.797028303146362, "time_total_s": 293.2585368156433, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 293.2585368156433, "iterations_since_restore": 39}
{"test__classification_error": 0.019999999552965164, "is_num_iterations_reached": 0, "done": false, "training_iteration": 40, "trial_id": "89ad9_00001", "date": "2024-06-30_18-40-53", "timestamp": 1719772853, "time_this_iter_s": 8.997281551361084, "time_total_s": 302.2558183670044, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 302.2558183670044, "iterations_since_restore": 40}
{"test__classification_error": 0.009999999776482582, "is_num_iterations_reached": 0, "done": false, "training_iteration": 41, "trial_id": "89ad9_00001", "date": "2024-06-30_18-40-59", "timestamp": 1719772859, "time_this_iter_s": 5.630841016769409, "time_total_s": 307.8866593837738, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 307.8866593837738, "iterations_since_restore": 41}
{"test__classification_error": 0.029999999329447746, "is_num_iterations_reached": 1, "done": true, "training_iteration": 42, "trial_id": "89ad9_00001", "date": "2024-06-30_18-41-07", "timestamp": 1719772867, "time_this_iter_s": 8.03209924697876, "time_total_s": 315.91875863075256, "pid": 1492, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 315.91875863075256, "iterations_since_restore": 42}
