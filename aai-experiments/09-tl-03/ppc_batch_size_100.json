{"test__classification_error": 0.3100000023841858, "is_num_iterations_reached": 0, "done": false, "training_iteration": 1, "trial_id": "b87ab_00001", "date": "2024-06-30_18-58-45", "timestamp": 1719773925, "time_this_iter_s": 11.357553720474243, "time_total_s": 11.357553720474243, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 11.357553720474243, "iterations_since_restore": 1}
{"test__classification_error": 0.2800000011920929, "is_num_iterations_reached": 0, "done": false, "training_iteration": 2, "trial_id": "b87ab_00001", "date": "2024-06-30_18-58-56", "timestamp": 1719773936, "time_this_iter_s": 10.534988641738892, "time_total_s": 21.892542362213135, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 21.892542362213135, "iterations_since_restore": 2}
{"test__classification_error": 0.26999998092651367, "is_num_iterations_reached": 0, "done": false, "training_iteration": 3, "trial_id": "b87ab_00001", "date": "2024-06-30_18-59-06", "timestamp": 1719773946, "time_this_iter_s": 10.255818367004395, "time_total_s": 32.14836072921753, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 32.14836072921753, "iterations_since_restore": 3}
{"test__classification_error": 0.2199999988079071, "is_num_iterations_reached": 0, "done": false, "training_iteration": 4, "trial_id": "b87ab_00001", "date": "2024-06-30_18-59-18", "timestamp": 1719773958, "time_this_iter_s": 12.011632442474365, "time_total_s": 44.159993171691895, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 44.159993171691895, "iterations_since_restore": 4}
{"test__classification_error": 0.20999999344348907, "is_num_iterations_reached": 0, "done": false, "training_iteration": 5, "trial_id": "b87ab_00001", "date": "2024-06-30_18-59-31", "timestamp": 1719773971, "time_this_iter_s": 12.849765062332153, "time_total_s": 57.00975823402405, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 57.00975823402405, "iterations_since_restore": 5}
{"test__classification_error": 0.19999998807907104, "is_num_iterations_reached": 0, "done": false, "training_iteration": 6, "trial_id": "b87ab_00001", "date": "2024-06-30_18-59-40", "timestamp": 1719773980, "time_this_iter_s": 9.560331583023071, "time_total_s": 66.57008981704712, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 66.57008981704712, "iterations_since_restore": 6}
{"test__classification_error": 0.22999998927116394, "is_num_iterations_reached": 0, "done": false, "training_iteration": 7, "trial_id": "b87ab_00001", "date": "2024-06-30_18-59-54", "timestamp": 1719773994, "time_this_iter_s": 14.099111795425415, "time_total_s": 80.66920161247253, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 80.66920161247253, "iterations_since_restore": 7}
{"test__classification_error": 0.2199999988079071, "is_num_iterations_reached": 0, "done": false, "training_iteration": 8, "trial_id": "b87ab_00001", "date": "2024-06-30_19-00-05", "timestamp": 1719774005, "time_this_iter_s": 10.605072736740112, "time_total_s": 91.27427434921265, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 91.27427434921265, "iterations_since_restore": 8}
{"test__classification_error": 0.1899999976158142, "is_num_iterations_reached": 0, "done": false, "training_iteration": 9, "trial_id": "b87ab_00001", "date": "2024-06-30_19-00-17", "timestamp": 1719774017, "time_this_iter_s": 11.703643083572388, "time_total_s": 102.97791743278503, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 102.97791743278503, "iterations_since_restore": 9}
{"test__classification_error": 0.22999998927116394, "is_num_iterations_reached": 0, "done": false, "training_iteration": 10, "trial_id": "b87ab_00001", "date": "2024-06-30_19-00-27", "timestamp": 1719774027, "time_this_iter_s": 10.666545391082764, "time_total_s": 113.6444628238678, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 113.6444628238678, "iterations_since_restore": 10}
{"test__classification_error": 0.1899999976158142, "is_num_iterations_reached": 0, "done": false, "training_iteration": 11, "trial_id": "b87ab_00001", "date": "2024-06-30_19-00-36", "timestamp": 1719774036, "time_this_iter_s": 8.8505277633667, "time_total_s": 122.4949905872345, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 122.4949905872345, "iterations_since_restore": 11}
{"test__classification_error": 0.2199999988079071, "is_num_iterations_reached": 0, "done": false, "training_iteration": 12, "trial_id": "b87ab_00001", "date": "2024-06-30_19-00-50", "timestamp": 1719774050, "time_this_iter_s": 13.485632181167603, "time_total_s": 135.9806227684021, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 135.9806227684021, "iterations_since_restore": 12}
{"test__classification_error": 0.14999999105930328, "is_num_iterations_reached": 0, "done": false, "training_iteration": 13, "trial_id": "b87ab_00001", "date": "2024-06-30_19-01-00", "timestamp": 1719774060, "time_this_iter_s": 10.57308578491211, "time_total_s": 146.5537085533142, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 146.5537085533142, "iterations_since_restore": 13}
{"test__classification_error": 0.17999999225139618, "is_num_iterations_reached": 0, "done": false, "training_iteration": 14, "trial_id": "b87ab_00001", "date": "2024-06-30_19-01-12", "timestamp": 1719774072, "time_this_iter_s": 11.765124559402466, "time_total_s": 158.31883311271667, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 158.31883311271667, "iterations_since_restore": 14}
{"test__classification_error": 0.1599999964237213, "is_num_iterations_reached": 0, "done": false, "training_iteration": 15, "trial_id": "b87ab_00001", "date": "2024-06-30_19-01-21", "timestamp": 1719774081, "time_this_iter_s": 9.199672222137451, "time_total_s": 167.51850533485413, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 167.51850533485413, "iterations_since_restore": 15}
{"test__classification_error": 0.1599999964237213, "is_num_iterations_reached": 0, "done": false, "training_iteration": 16, "trial_id": "b87ab_00001", "date": "2024-06-30_19-01-35", "timestamp": 1719774095, "time_this_iter_s": 13.744817972183228, "time_total_s": 181.26332330703735, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 181.26332330703735, "iterations_since_restore": 16}
{"test__classification_error": 0.20999999344348907, "is_num_iterations_reached": 0, "done": false, "training_iteration": 17, "trial_id": "b87ab_00001", "date": "2024-06-30_19-01-47", "timestamp": 1719774107, "time_this_iter_s": 11.634878873825073, "time_total_s": 192.89820218086243, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 192.89820218086243, "iterations_since_restore": 17}
{"test__classification_error": 0.17999999225139618, "is_num_iterations_reached": 0, "done": false, "training_iteration": 18, "trial_id": "b87ab_00001", "date": "2024-06-30_19-02-00", "timestamp": 1719774120, "time_this_iter_s": 13.472380638122559, "time_total_s": 206.37058281898499, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 206.37058281898499, "iterations_since_restore": 18}
{"test__classification_error": 0.19999998807907104, "is_num_iterations_reached": 0, "done": false, "training_iteration": 19, "trial_id": "b87ab_00001", "date": "2024-06-30_19-02-12", "timestamp": 1719774132, "time_this_iter_s": 11.252137184143066, "time_total_s": 217.62272000312805, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 217.62272000312805, "iterations_since_restore": 19}
{"test__classification_error": 0.17000000178813934, "is_num_iterations_reached": 0, "done": false, "training_iteration": 20, "trial_id": "b87ab_00001", "date": "2024-06-30_19-02-23", "timestamp": 1719774143, "time_this_iter_s": 11.904721021652222, "time_total_s": 229.52744102478027, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 229.52744102478027, "iterations_since_restore": 20}
{"test__classification_error": 0.1599999964237213, "is_num_iterations_reached": 0, "done": false, "training_iteration": 21, "trial_id": "b87ab_00001", "date": "2024-06-30_19-02-33", "timestamp": 1719774153, "time_this_iter_s": 9.115446090698242, "time_total_s": 238.64288711547852, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 238.64288711547852, "iterations_since_restore": 21}
{"test__classification_error": 0.20999999344348907, "is_num_iterations_reached": 0, "done": false, "training_iteration": 22, "trial_id": "b87ab_00001", "date": "2024-06-30_19-02-41", "timestamp": 1719774161, "time_this_iter_s": 7.982609272003174, "time_total_s": 246.6254963874817, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 246.6254963874817, "iterations_since_restore": 22}
{"test__classification_error": 0.1599999964237213, "is_num_iterations_reached": 0, "done": false, "training_iteration": 23, "trial_id": "b87ab_00001", "date": "2024-06-30_19-02-51", "timestamp": 1719774171, "time_this_iter_s": 10.652438402175903, "time_total_s": 257.2779347896576, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 257.2779347896576, "iterations_since_restore": 23}
{"test__classification_error": 0.19999998807907104, "is_num_iterations_reached": 0, "done": false, "training_iteration": 24, "trial_id": "b87ab_00001", "date": "2024-06-30_19-03-04", "timestamp": 1719774184, "time_this_iter_s": 12.63714599609375, "time_total_s": 269.91508078575134, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 269.91508078575134, "iterations_since_restore": 24}
{"test__classification_error": 0.17999999225139618, "is_num_iterations_reached": 0, "done": false, "training_iteration": 25, "trial_id": "b87ab_00001", "date": "2024-06-30_19-03-15", "timestamp": 1719774195, "time_this_iter_s": 10.605045795440674, "time_total_s": 280.520126581192, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 280.520126581192, "iterations_since_restore": 25}
{"test__classification_error": 0.1899999976158142, "is_num_iterations_reached": 0, "done": false, "training_iteration": 26, "trial_id": "b87ab_00001", "date": "2024-06-30_19-03-26", "timestamp": 1719774206, "time_this_iter_s": 11.258678436279297, "time_total_s": 291.7788050174713, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 291.7788050174713, "iterations_since_restore": 26}
{"test__classification_error": 0.22999998927116394, "is_num_iterations_reached": 0, "done": false, "training_iteration": 27, "trial_id": "b87ab_00001", "date": "2024-06-30_19-03-37", "timestamp": 1719774217, "time_this_iter_s": 10.71582579612732, "time_total_s": 302.49463081359863, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 302.49463081359863, "iterations_since_restore": 27}
{"test__classification_error": 0.23999999463558197, "is_num_iterations_reached": 0, "done": false, "training_iteration": 28, "trial_id": "b87ab_00001", "date": "2024-06-30_19-03-47", "timestamp": 1719774227, "time_this_iter_s": 10.908470869064331, "time_total_s": 313.40310168266296, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 313.40310168266296, "iterations_since_restore": 28}
{"test__classification_error": 0.17999999225139618, "is_num_iterations_reached": 0, "done": false, "training_iteration": 29, "trial_id": "b87ab_00001", "date": "2024-06-30_19-03-58", "timestamp": 1719774238, "time_this_iter_s": 10.679786205291748, "time_total_s": 324.0828878879547, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 324.0828878879547, "iterations_since_restore": 29}
{"test__classification_error": 0.1599999964237213, "is_num_iterations_reached": 0, "done": false, "training_iteration": 30, "trial_id": "b87ab_00001", "date": "2024-06-30_19-04-10", "timestamp": 1719774250, "time_this_iter_s": 11.941664457321167, "time_total_s": 336.0245523452759, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 336.0245523452759, "iterations_since_restore": 30}
{"test__classification_error": 0.19999998807907104, "is_num_iterations_reached": 0, "done": false, "training_iteration": 31, "trial_id": "b87ab_00001", "date": "2024-06-30_19-04-21", "timestamp": 1719774261, "time_this_iter_s": 10.854206800460815, "time_total_s": 346.8787591457367, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 346.8787591457367, "iterations_since_restore": 31}
{"test__classification_error": 0.22999998927116394, "is_num_iterations_reached": 0, "done": false, "training_iteration": 32, "trial_id": "b87ab_00001", "date": "2024-06-30_19-04-34", "timestamp": 1719774274, "time_this_iter_s": 13.28422498703003, "time_total_s": 360.1629841327667, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 360.1629841327667, "iterations_since_restore": 32}
{"test__classification_error": 0.20999999344348907, "is_num_iterations_reached": 0, "done": false, "training_iteration": 33, "trial_id": "b87ab_00001", "date": "2024-06-30_19-04-45", "timestamp": 1719774285, "time_this_iter_s": 10.382201194763184, "time_total_s": 370.5451853275299, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 370.5451853275299, "iterations_since_restore": 33}
{"test__classification_error": 0.17000000178813934, "is_num_iterations_reached": 0, "done": false, "training_iteration": 34, "trial_id": "b87ab_00001", "date": "2024-06-30_19-04-55", "timestamp": 1719774295, "time_this_iter_s": 10.194650411605835, "time_total_s": 380.73983573913574, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 380.73983573913574, "iterations_since_restore": 34}
{"test__classification_error": 0.17000000178813934, "is_num_iterations_reached": 0, "done": false, "training_iteration": 35, "trial_id": "b87ab_00001", "date": "2024-06-30_19-05-09", "timestamp": 1719774309, "time_this_iter_s": 13.677518844604492, "time_total_s": 394.41735458374023, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 394.41735458374023, "iterations_since_restore": 35}
{"test__classification_error": 0.17000000178813934, "is_num_iterations_reached": 0, "done": false, "training_iteration": 36, "trial_id": "b87ab_00001", "date": "2024-06-30_19-05-19", "timestamp": 1719774319, "time_this_iter_s": 10.734057903289795, "time_total_s": 405.15141248703003, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 405.15141248703003, "iterations_since_restore": 36}
{"test__classification_error": 0.1899999976158142, "is_num_iterations_reached": 0, "done": false, "training_iteration": 37, "trial_id": "b87ab_00001", "date": "2024-06-30_19-05-30", "timestamp": 1719774330, "time_this_iter_s": 10.449156284332275, "time_total_s": 415.6005687713623, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 415.6005687713623, "iterations_since_restore": 37}
{"test__classification_error": 0.14999999105930328, "is_num_iterations_reached": 0, "done": false, "training_iteration": 38, "trial_id": "b87ab_00001", "date": "2024-06-30_19-05-39", "timestamp": 1719774339, "time_this_iter_s": 9.162822723388672, "time_total_s": 424.763391494751, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 424.763391494751, "iterations_since_restore": 38}
{"test__classification_error": 0.1599999964237213, "is_num_iterations_reached": 0, "done": false, "training_iteration": 39, "trial_id": "b87ab_00001", "date": "2024-06-30_19-05-50", "timestamp": 1719774350, "time_this_iter_s": 11.356590032577515, "time_total_s": 436.1199815273285, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 436.1199815273285, "iterations_since_restore": 39}
{"test__classification_error": 0.14999999105930328, "is_num_iterations_reached": 0, "done": false, "training_iteration": 40, "trial_id": "b87ab_00001", "date": "2024-06-30_19-06-00", "timestamp": 1719774360, "time_this_iter_s": 9.271045923233032, "time_total_s": 445.3910274505615, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 445.3910274505615, "iterations_since_restore": 40}
{"test__classification_error": 0.14000000059604645, "is_num_iterations_reached": 0, "done": false, "training_iteration": 41, "trial_id": "b87ab_00001", "date": "2024-06-30_19-06-10", "timestamp": 1719774370, "time_this_iter_s": 10.151479721069336, "time_total_s": 455.54250717163086, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 455.54250717163086, "iterations_since_restore": 41}
{"test__classification_error": 0.14000000059604645, "is_num_iterations_reached": 1, "done": true, "training_iteration": 42, "trial_id": "b87ab_00001", "date": "2024-06-30_19-06-21", "timestamp": 1719774381, "time_this_iter_s": 11.550325393676758, "time_total_s": 467.0928325653076, "pid": 9262, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.3, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 100, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": true, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path=f'{os.environ.get(\"PT_MODEL_DIR\")}/pt_ppc.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_ppc.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 467.0928325653076, "iterations_since_restore": 42}
