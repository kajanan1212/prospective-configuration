{"test__classification_error": 0.1515151560306549, "is_num_iterations_reached": 0, "done": false, "training_iteration": 1, "trial_id": "89ad9_00000", "date": "2024-06-30_18-36-01", "timestamp": 1719772561, "time_this_iter_s": 11.548717737197876, "time_total_s": 11.548717737197876, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 11.548717737197876, "iterations_since_restore": 1}
{"test__classification_error": 0.05303030461072922, "is_num_iterations_reached": 0, "done": false, "training_iteration": 2, "trial_id": "89ad9_00000", "date": "2024-06-30_18-36-11", "timestamp": 1719772571, "time_this_iter_s": 9.541300535202026, "time_total_s": 21.090018272399902, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 21.090018272399902, "iterations_since_restore": 2}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 3, "trial_id": "89ad9_00000", "date": "2024-06-30_18-36-20", "timestamp": 1719772580, "time_this_iter_s": 9.40878963470459, "time_total_s": 30.498807907104492, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 30.498807907104492, "iterations_since_restore": 3}
{"test__classification_error": 0.04545454680919647, "is_num_iterations_reached": 0, "done": false, "training_iteration": 4, "trial_id": "89ad9_00000", "date": "2024-06-30_18-36-30", "timestamp": 1719772590, "time_this_iter_s": 9.531229257583618, "time_total_s": 40.03003716468811, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 40.03003716468811, "iterations_since_restore": 4}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 5, "trial_id": "89ad9_00000", "date": "2024-06-30_18-36-38", "timestamp": 1719772598, "time_this_iter_s": 8.444849014282227, "time_total_s": 48.47488617897034, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 48.47488617897034, "iterations_since_restore": 5}
{"test__classification_error": 0.022727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 6, "trial_id": "89ad9_00000", "date": "2024-06-30_18-36-48", "timestamp": 1719772608, "time_this_iter_s": 9.879757165908813, "time_total_s": 58.35464334487915, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 58.35464334487915, "iterations_since_restore": 6}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 7, "trial_id": "89ad9_00000", "date": "2024-06-30_18-36-56", "timestamp": 1719772616, "time_this_iter_s": 7.572399616241455, "time_total_s": 65.9270429611206, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 65.9270429611206, "iterations_since_restore": 7}
{"test__classification_error": 0.022727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 8, "trial_id": "89ad9_00000", "date": "2024-06-30_18-37-06", "timestamp": 1719772626, "time_this_iter_s": 9.766788959503174, "time_total_s": 75.69383192062378, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 75.69383192062378, "iterations_since_restore": 8}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 9, "trial_id": "89ad9_00000", "date": "2024-06-30_18-37-13", "timestamp": 1719772633, "time_this_iter_s": 7.523770332336426, "time_total_s": 83.2176022529602, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 83.2176022529602, "iterations_since_restore": 9}
{"test__classification_error": 0.022727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 10, "trial_id": "89ad9_00000", "date": "2024-06-30_18-37-23", "timestamp": 1719772643, "time_this_iter_s": 9.535274505615234, "time_total_s": 92.75287675857544, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 92.75287675857544, "iterations_since_restore": 10}
{"test__classification_error": 0.01515151560306549, "is_num_iterations_reached": 0, "done": false, "training_iteration": 11, "trial_id": "89ad9_00000", "date": "2024-06-30_18-37-30", "timestamp": 1719772650, "time_this_iter_s": 7.712682485580444, "time_total_s": 100.46555924415588, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 100.46555924415588, "iterations_since_restore": 11}
{"test__classification_error": 0.022727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 12, "trial_id": "89ad9_00000", "date": "2024-06-30_18-37-41", "timestamp": 1719772661, "time_this_iter_s": 10.239375591278076, "time_total_s": 110.70493483543396, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 110.70493483543396, "iterations_since_restore": 12}
{"test__classification_error": 0.022727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 13, "trial_id": "89ad9_00000", "date": "2024-06-30_18-37-49", "timestamp": 1719772669, "time_this_iter_s": 8.925762176513672, "time_total_s": 119.63069701194763, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 119.63069701194763, "iterations_since_restore": 13}
{"test__classification_error": 0.022727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 14, "trial_id": "89ad9_00000", "date": "2024-06-30_18-37-58", "timestamp": 1719772678, "time_this_iter_s": 8.793336153030396, "time_total_s": 128.42403316497803, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 128.42403316497803, "iterations_since_restore": 14}
{"test__classification_error": 0.03030303120613098, "is_num_iterations_reached": 0, "done": false, "training_iteration": 15, "trial_id": "89ad9_00000", "date": "2024-06-30_18-38-08", "timestamp": 1719772688, "time_this_iter_s": 9.844637632369995, "time_total_s": 138.26867079734802, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 138.26867079734802, "iterations_since_restore": 15}
{"test__classification_error": 0.03030303120613098, "is_num_iterations_reached": 0, "done": false, "training_iteration": 16, "trial_id": "89ad9_00000", "date": "2024-06-30_18-38-19", "timestamp": 1719772699, "time_this_iter_s": 10.922951221466064, "time_total_s": 149.1916220188141, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 149.1916220188141, "iterations_since_restore": 16}
{"test__classification_error": 0.03030303120613098, "is_num_iterations_reached": 0, "done": false, "training_iteration": 17, "trial_id": "89ad9_00000", "date": "2024-06-30_18-38-29", "timestamp": 1719772709, "time_this_iter_s": 10.083850145339966, "time_total_s": 159.27547216415405, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 159.27547216415405, "iterations_since_restore": 17}
{"test__classification_error": 0.022727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 18, "trial_id": "89ad9_00000", "date": "2024-06-30_18-38-38", "timestamp": 1719772718, "time_this_iter_s": 8.386405944824219, "time_total_s": 167.66187810897827, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 167.66187810897827, "iterations_since_restore": 18}
{"test__classification_error": 0.022727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 19, "trial_id": "89ad9_00000", "date": "2024-06-30_18-38-47", "timestamp": 1719772727, "time_this_iter_s": 9.20621943473816, "time_total_s": 176.86809754371643, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 176.86809754371643, "iterations_since_restore": 19}
{"test__classification_error": 0.03030303120613098, "is_num_iterations_reached": 0, "done": false, "training_iteration": 20, "trial_id": "89ad9_00000", "date": "2024-06-30_18-38-56", "timestamp": 1719772736, "time_this_iter_s": 8.997088432312012, "time_total_s": 185.86518597602844, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 185.86518597602844, "iterations_since_restore": 20}
{"test__classification_error": 0.022727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 21, "trial_id": "89ad9_00000", "date": "2024-06-30_18-39-04", "timestamp": 1719772744, "time_this_iter_s": 8.404344081878662, "time_total_s": 194.2695300579071, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 194.2695300579071, "iterations_since_restore": 21}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 22, "trial_id": "89ad9_00000", "date": "2024-06-30_18-39-15", "timestamp": 1719772755, "time_this_iter_s": 10.323302030563354, "time_total_s": 204.59283208847046, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 204.59283208847046, "iterations_since_restore": 22}
{"test__classification_error": 0.01515151560306549, "is_num_iterations_reached": 0, "done": false, "training_iteration": 23, "trial_id": "89ad9_00000", "date": "2024-06-30_18-39-23", "timestamp": 1719772763, "time_this_iter_s": 8.226369380950928, "time_total_s": 212.8192014694214, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 212.8192014694214, "iterations_since_restore": 23}
{"test__classification_error": 0.022727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 24, "trial_id": "89ad9_00000", "date": "2024-06-30_18-39-33", "timestamp": 1719772773, "time_this_iter_s": 10.2889404296875, "time_total_s": 223.1081418991089, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 223.1081418991089, "iterations_since_restore": 24}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 25, "trial_id": "89ad9_00000", "date": "2024-06-30_18-39-40", "timestamp": 1719772780, "time_this_iter_s": 7.214466333389282, "time_total_s": 230.32260823249817, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 230.32260823249817, "iterations_since_restore": 25}
{"test__classification_error": 0.03030303120613098, "is_num_iterations_reached": 0, "done": false, "training_iteration": 26, "trial_id": "89ad9_00000", "date": "2024-06-30_18-39-50", "timestamp": 1719772790, "time_this_iter_s": 10.08766222000122, "time_total_s": 240.4102704524994, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 240.4102704524994, "iterations_since_restore": 26}
{"test__classification_error": 0.03030303120613098, "is_num_iterations_reached": 0, "done": false, "training_iteration": 27, "trial_id": "89ad9_00000", "date": "2024-06-30_18-39-59", "timestamp": 1719772799, "time_this_iter_s": 8.937622785568237, "time_total_s": 249.34789323806763, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 249.34789323806763, "iterations_since_restore": 27}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 28, "trial_id": "89ad9_00000", "date": "2024-06-30_18-40-09", "timestamp": 1719772809, "time_this_iter_s": 9.205441951751709, "time_total_s": 258.55333518981934, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 258.55333518981934, "iterations_since_restore": 28}
{"test__classification_error": 0.03030303120613098, "is_num_iterations_reached": 0, "done": false, "training_iteration": 29, "trial_id": "89ad9_00000", "date": "2024-06-30_18-40-18", "timestamp": 1719772818, "time_this_iter_s": 9.055540800094604, "time_total_s": 267.60887598991394, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 267.60887598991394, "iterations_since_restore": 29}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 30, "trial_id": "89ad9_00000", "date": "2024-06-30_18-40-26", "timestamp": 1719772826, "time_this_iter_s": 8.277645587921143, "time_total_s": 275.8865215778351, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 275.8865215778351, "iterations_since_restore": 30}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 31, "trial_id": "89ad9_00000", "date": "2024-06-30_18-40-36", "timestamp": 1719772836, "time_this_iter_s": 10.421767711639404, "time_total_s": 286.3082892894745, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 286.3082892894745, "iterations_since_restore": 31}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 32, "trial_id": "89ad9_00000", "date": "2024-06-30_18-40-47", "timestamp": 1719772847, "time_this_iter_s": 10.687546968460083, "time_total_s": 296.99583625793457, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 296.99583625793457, "iterations_since_restore": 32}
{"test__classification_error": 0.022727273404598236, "is_num_iterations_reached": 0, "done": false, "training_iteration": 33, "trial_id": "89ad9_00000", "date": "2024-06-30_18-40-57", "timestamp": 1719772857, "time_this_iter_s": 10.070587873458862, "time_total_s": 307.06642413139343, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 307.06642413139343, "iterations_since_restore": 33}
{"test__classification_error": 0.03030303120613098, "is_num_iterations_reached": 0, "done": false, "training_iteration": 34, "trial_id": "89ad9_00000", "date": "2024-06-30_18-41-07", "timestamp": 1719772867, "time_this_iter_s": 9.392162561416626, "time_total_s": 316.45858669281006, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 316.45858669281006, "iterations_since_restore": 34}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 35, "trial_id": "89ad9_00000", "date": "2024-06-30_18-41-13", "timestamp": 1719772873, "time_this_iter_s": 6.41253924369812, "time_total_s": 322.8711259365082, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 322.8711259365082, "iterations_since_restore": 35}
{"test__classification_error": 0.03030303120613098, "is_num_iterations_reached": 0, "done": false, "training_iteration": 36, "trial_id": "89ad9_00000", "date": "2024-06-30_18-41-18", "timestamp": 1719772878, "time_this_iter_s": 5.171759605407715, "time_total_s": 328.0428855419159, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 328.0428855419159, "iterations_since_restore": 36}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 37, "trial_id": "89ad9_00000", "date": "2024-06-30_18-41-25", "timestamp": 1719772885, "time_this_iter_s": 6.730855464935303, "time_total_s": 334.7737410068512, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 334.7737410068512, "iterations_since_restore": 37}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 38, "trial_id": "89ad9_00000", "date": "2024-06-30_18-41-30", "timestamp": 1719772890, "time_this_iter_s": 4.822542428970337, "time_total_s": 339.59628343582153, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 339.59628343582153, "iterations_since_restore": 38}
{"test__classification_error": 0.03787878900766373, "is_num_iterations_reached": 0, "done": false, "training_iteration": 39, "trial_id": "89ad9_00000", "date": "2024-06-30_18-41-35", "timestamp": 1719772895, "time_this_iter_s": 5.688997268676758, "time_total_s": 345.2852807044983, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 345.2852807044983, "iterations_since_restore": 39}
{"test__classification_error": 0.03030303120613098, "is_num_iterations_reached": 0, "done": false, "training_iteration": 40, "trial_id": "89ad9_00000", "date": "2024-06-30_18-41-41", "timestamp": 1719772901, "time_this_iter_s": 6.03968620300293, "time_total_s": 351.3249669075012, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 351.3249669075012, "iterations_since_restore": 40}
{"test__classification_error": 0.03030303120613098, "is_num_iterations_reached": 0, "done": false, "training_iteration": 41, "trial_id": "89ad9_00000", "date": "2024-06-30_18-41-46", "timestamp": 1719772906, "time_this_iter_s": 4.8935606479644775, "time_total_s": 356.2185275554657, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 356.2185275554657, "iterations_since_restore": 41}
{"test__classification_error": 0.04545454680919647, "is_num_iterations_reached": 1, "done": true, "training_iteration": 42, "trial_id": "89ad9_00000", "date": "2024-06-30_18-41-52", "timestamp": 1719772912, "time_this_iter_s": 6.060576677322388, "time_total_s": 362.2791042327881, "pid": 1490, "hostname": "ac633632a2d9", "node_ip": "172.28.0.12", "config": {"version": 1.0, "device": "torch.device('cuda')", "seed": 1482555873, "num_iterations": 42, "dataset": "covid-19-lung-ct-scans", "batch_size": 33, "before_DatasetLearningTrainable_setup_code": "def data_loader_fn(dataset, train, batch_size, partial_num=-1):\n    \n    transform = []\n    transform.append(transforms.Grayscale(3))\n    transform.append(transforms.Resize((32, 32)))\n    transform.append(transforms.ToTensor())\n\n    target_transform = []\n    target_transform.append(\n        transforms.Lambda(\n            lambda idx: utils.np_idx2onehot(idx, 2)\n        )\n    )\n\n    dataset = datasets.ImageFolder(\n        f'{os.environ.get(\"DATA_DIR\")}/{dataset}',\n        transform=transforms.Compose(transform),\n        target_transform=transforms.Compose(target_transform)\n    )\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n    if train:\n        data_subset = train_dataset\n    else:\n        data_subset = test_dataset\n\n    return DataLoader(\n        data_subset,\n        batch_size=batch_size,\n        num_workers=1,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True,\n    )", "data_packs": {"train": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=True,\n    batch_size=self.config['batch_size'],\n)", "do": "['learn']"}, "test": {"data_loader": "data_loader_fn(\n    dataset=self.config['dataset'],\n    train=False,\n    batch_size=self.config['batch_size'],\n    # debug\n    # partial_num=100,\n)", "do": "['predict']"}}, "predictive_coding": false, "PCTrainer_kwargs": {"update_x_at": "all", "optimizer_x_fn": "<class 'torch.optim.sgd.SGD'>", "optimizer_x_kwargs": {"lr": 0.5}, "x_lr_discount": 0.5, "x_lr_amplifier": 1.0, "update_p_at": "all", "optimizer_p_fn": "<class 'torch.optim.adam.Adam'>", "optimizer_p_kwargs": {"lr": 0.0001, "weight_decay": 0.01}, "T": 16, "plot_progress_at": []}, "model": {"acf": "torch.nn.ReLU", "model_type_order": "['Weights', 'PCLayer', 'Acf', 'MaxPool']", "cnn_layers": {"cnn_0": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 3, "out_channels": 64, "kernel_size": 5, "stride": 1, "padding": 2, "bias": false}}, "cnn_1": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 64, "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}, "cnn_2": {"fn": "torch.nn.Conv2d", "kwargs": {"in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1, "bias": false}}}, "linear_layers": {"linear_0": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 4096, "out_features": 512, "bias": true}}, "last": {"fn": "torch.nn.Linear", "kwargs": {"in_features": 512, "out_features": 10, "bias": true}}}}, "model_creation_code": "# import\nimport predictive_coding as pc\nimport torch.optim as optim\nimport aai_utils as u\n\n# create model\nself.model = u.create_model(\n    self.config['predictive_coding'],\n    **self.config['model'],\n    pt_model_path='/content/drive/MyDrive/results/08-pt-final/model/pt_rbp.pth',\n    trainable_layers=4,\n).to(self.device)\n\n# create pc_trainer kwargs\nself.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])\n)\nself.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(\n    'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])\n)\nself.config['PCTrainer_kwargs']['plot_progress_at']=eval(\n    self.config['PCTrainer_kwargs']['plot_progress_at']\n)\n\n# create pc_trainer\nself.pc_trainer = pc.PCTrainer(\n    self.model,\n    **self.config['PCTrainer_kwargs'],\n)", "predict_code": "\nself.model.eval()\nprediction = self.model(data)\nself.classification_error = utils.get_classification_error(\n    prediction, target\n)", "train_on_batch_kwargs": {"is_log_progress": false, "is_return_results_every_t": false, "is_checking_after_callback_after_t": false}, "learn_code": "self.model.train()\n\ndef loss_fn(outputs, target):\n    return (outputs - target).pow(2).sum() * 0.5\n\nself.pc_trainer.train_on_batch(\n    data, loss_fn,\n    loss_fn_kwargs={\n        'target': target,\n    },\n    **self.config['train_on_batch_kwargs'],\n)\n\ncurr_working_dir = os.getcwd()\nmodel_filename = 'pt_rbp.pth'\n\ncurr_model_path = os.path.join(curr_working_dir, model_filename)\ntorch.save(self.model.state_dict(), curr_model_path)\n\nfinal_model_path = os.path.join(os.environ.get('MODEL_DIR'), model_filename)\nshutil.copy(curr_model_path, final_model_path)", "log_packs": {"classification_error": {"log": "self.classification_error.item()", "at_data_pack": "['test']"}}}, "time_since_restore": 362.2791042327881, "iterations_since_restore": 42}
